"""Backward Triton kernels for Manifold-Constrained Hyper-Connections.

Key optimization: Recomputation during backward to avoid storing intermediate states.
"""

import torch
import triton
import triton.language as tl
from typing import Tuple


# -----------------------------------------------------------------------------
# Sinkhorn Backward Kernel
# Recomputes forward pass to save memory (20x reduction)
# -----------------------------------------------------------------------------

@triton.jit
def _sinkhorn_backward_kernel(
    M_orig_ptr, dP_ptr, dM_ptr,
    batch,
    num_iters: tl.constexpr,
    eps: tl.constexpr,
    NUM_STREAMS: tl.constexpr,
):
    """Backward through Sinkhorn with O(TÂ²) recomputation."""
    pid = tl.program_id(0)
    if pid >= batch:
        return

    base = pid * 16

    # Load original input and compute sign for abs() backward
    m00_orig = tl.load(M_orig_ptr + base + 0).to(tl.float32)
    m01_orig = tl.load(M_orig_ptr + base + 1).to(tl.float32)
    m02_orig = tl.load(M_orig_ptr + base + 2).to(tl.float32)
    m03_orig = tl.load(M_orig_ptr + base + 3).to(tl.float32)
    m10_orig = tl.load(M_orig_ptr + base + 4).to(tl.float32)
    m11_orig = tl.load(M_orig_ptr + base + 5).to(tl.float32)
    m12_orig = tl.load(M_orig_ptr + base + 6).to(tl.float32)
    m13_orig = tl.load(M_orig_ptr + base + 7).to(tl.float32)
    m20_orig = tl.load(M_orig_ptr + base + 8).to(tl.float32)
    m21_orig = tl.load(M_orig_ptr + base + 9).to(tl.float32)
    m22_orig = tl.load(M_orig_ptr + base + 10).to(tl.float32)
    m23_orig = tl.load(M_orig_ptr + base + 11).to(tl.float32)
    m30_orig = tl.load(M_orig_ptr + base + 12).to(tl.float32)
    m31_orig = tl.load(M_orig_ptr + base + 13).to(tl.float32)
    m32_orig = tl.load(M_orig_ptr + base + 14).to(tl.float32)
    m33_orig = tl.load(M_orig_ptr + base + 15).to(tl.float32)

    # Sign for abs() backward
    s00 = tl.where(m00_orig >= 0, 1.0, -1.0)
    s01 = tl.where(m01_orig >= 0, 1.0, -1.0)
    s02 = tl.where(m02_orig >= 0, 1.0, -1.0)
    s03 = tl.where(m03_orig >= 0, 1.0, -1.0)
    s10 = tl.where(m10_orig >= 0, 1.0, -1.0)
    s11 = tl.where(m11_orig >= 0, 1.0, -1.0)
    s12 = tl.where(m12_orig >= 0, 1.0, -1.0)
    s13 = tl.where(m13_orig >= 0, 1.0, -1.0)
    s20 = tl.where(m20_orig >= 0, 1.0, -1.0)
    s21 = tl.where(m21_orig >= 0, 1.0, -1.0)
    s22 = tl.where(m22_orig >= 0, 1.0, -1.0)
    s23 = tl.where(m23_orig >= 0, 1.0, -1.0)
    s30 = tl.where(m30_orig >= 0, 1.0, -1.0)
    s31 = tl.where(m31_orig >= 0, 1.0, -1.0)
    s32 = tl.where(m32_orig >= 0, 1.0, -1.0)
    s33 = tl.where(m33_orig >= 0, 1.0, -1.0)

    # Load upstream gradient
    d00 = tl.load(dP_ptr + base + 0).to(tl.float32)
    d01 = tl.load(dP_ptr + base + 1).to(tl.float32)
    d02 = tl.load(dP_ptr + base + 2).to(tl.float32)
    d03 = tl.load(dP_ptr + base + 3).to(tl.float32)
    d10 = tl.load(dP_ptr + base + 4).to(tl.float32)
    d11 = tl.load(dP_ptr + base + 5).to(tl.float32)
    d12 = tl.load(dP_ptr + base + 6).to(tl.float32)
    d13 = tl.load(dP_ptr + base + 7).to(tl.float32)
    d20 = tl.load(dP_ptr + base + 8).to(tl.float32)
    d21 = tl.load(dP_ptr + base + 9).to(tl.float32)
    d22 = tl.load(dP_ptr + base + 10).to(tl.float32)
    d23 = tl.load(dP_ptr + base + 11).to(tl.float32)
    d30 = tl.load(dP_ptr + base + 12).to(tl.float32)
    d31 = tl.load(dP_ptr + base + 13).to(tl.float32)
    d32 = tl.load(dP_ptr + base + 14).to(tl.float32)
    d33 = tl.load(dP_ptr + base + 15).to(tl.float32)

    # Backward through iterations (reverse order with recomputation)
    for _iter in range(num_iters):
        target_iter = num_iters - 1 - _iter

        # Recompute forward to target state
        m00 = tl.abs(m00_orig) + eps
        m01 = tl.abs(m01_orig) + eps
        m02 = tl.abs(m02_orig) + eps
        m03 = tl.abs(m03_orig) + eps
        m10 = tl.abs(m10_orig) + eps
        m11 = tl.abs(m11_orig) + eps
        m12 = tl.abs(m12_orig) + eps
        m13 = tl.abs(m13_orig) + eps
        m20 = tl.abs(m20_orig) + eps
        m21 = tl.abs(m21_orig) + eps
        m22 = tl.abs(m22_orig) + eps
        m23 = tl.abs(m23_orig) + eps
        m30 = tl.abs(m30_orig) + eps
        m31 = tl.abs(m31_orig) + eps
        m32 = tl.abs(m32_orig) + eps
        m33 = tl.abs(m33_orig) + eps

        for _fwd in range(num_iters):
            if _fwd < target_iter:
                r0 = m00 + m01 + m02 + m03 + eps
                r1 = m10 + m11 + m12 + m13 + eps
                r2 = m20 + m21 + m22 + m23 + eps
                r3 = m30 + m31 + m32 + m33 + eps
                m00 /= r0; m01 /= r0; m02 /= r0; m03 /= r0
                m10 /= r1; m11 /= r1; m12 /= r1; m13 /= r1
                m20 /= r2; m21 /= r2; m22 /= r2; m23 /= r2
                m30 /= r3; m31 /= r3; m32 /= r3; m33 /= r3

                c0 = m00 + m10 + m20 + m30 + eps
                c1 = m01 + m11 + m21 + m31 + eps
                c2 = m02 + m12 + m22 + m32 + eps
                c3 = m03 + m13 + m23 + m33 + eps
                m00 /= c0; m10 /= c0; m20 /= c0; m30 /= c0
                m01 /= c1; m11 /= c1; m21 /= c1; m31 /= c1
                m02 /= c2; m12 /= c2; m22 /= c2; m32 /= c2
                m03 /= c3; m13 /= c3; m23 /= c3; m33 /= c3

        # Forward step at target_iter
        r0 = m00 + m01 + m02 + m03 + eps
        r1 = m10 + m11 + m12 + m13 + eps
        r2 = m20 + m21 + m22 + m23 + eps
        r3 = m30 + m31 + m32 + m33 + eps

        mr00 = m00 / r0; mr01 = m01 / r0; mr02 = m02 / r0; mr03 = m03 / r0
        mr10 = m10 / r1; mr11 = m11 / r1; mr12 = m12 / r1; mr13 = m13 / r1
        mr20 = m20 / r2; mr21 = m21 / r2; mr22 = m22 / r2; mr23 = m23 / r2
        mr30 = m30 / r3; mr31 = m31 / r3; mr32 = m32 / r3; mr33 = m33 / r3

        c0 = mr00 + mr10 + mr20 + mr30 + eps
        c1 = mr01 + mr11 + mr21 + mr31 + eps
        c2 = mr02 + mr12 + mr22 + mr32 + eps
        c3 = mr03 + mr13 + mr23 + mr33 + eps

        mc00 = mr00 / c0; mc10 = mr10 / c0; mc20 = mr20 / c0; mc30 = mr30 / c0
        mc01 = mr01 / c1; mc11 = mr11 / c1; mc21 = mr21 / c1; mc31 = mr31 / c1
        mc02 = mr02 / c2; mc12 = mr12 / c2; mc22 = mr22 / c2; mc32 = mr32 / c2
        mc03 = mr03 / c3; mc13 = mr13 / c3; mc23 = mr23 / c3; mc33 = mr33 / c3

        # Backward through column norm: dx_i = (dy_i - <dy, y>) / s
        dot0 = d00 * mc00 + d10 * mc10 + d20 * mc20 + d30 * mc30
        dot1 = d01 * mc01 + d11 * mc11 + d21 * mc21 + d31 * mc31
        dot2 = d02 * mc02 + d12 * mc12 + d22 * mc22 + d32 * mc32
        dot3 = d03 * mc03 + d13 * mc13 + d23 * mc23 + d33 * mc33

        dr00 = (d00 - dot0) / c0; dr10 = (d10 - dot0) / c0
        dr20 = (d20 - dot0) / c0; dr30 = (d30 - dot0) / c0
        dr01 = (d01 - dot1) / c1; dr11 = (d11 - dot1) / c1
        dr21 = (d21 - dot1) / c1; dr31 = (d31 - dot1) / c1
        dr02 = (d02 - dot2) / c2; dr12 = (d12 - dot2) / c2
        dr22 = (d22 - dot2) / c2; dr32 = (d32 - dot2) / c2
        dr03 = (d03 - dot3) / c3; dr13 = (d13 - dot3) / c3
        dr23 = (d23 - dot3) / c3; dr33 = (d33 - dot3) / c3

        # Backward through row norm
        dot0 = dr00 * mr00 + dr01 * mr01 + dr02 * mr02 + dr03 * mr03
        dot1 = dr10 * mr10 + dr11 * mr11 + dr12 * mr12 + dr13 * mr13
        dot2 = dr20 * mr20 + dr21 * mr21 + dr22 * mr22 + dr23 * mr23
        dot3 = dr30 * mr30 + dr31 * mr31 + dr32 * mr32 + dr33 * mr33

        d00 = (dr00 - dot0) / r0; d01 = (dr01 - dot0) / r0
        d02 = (dr02 - dot0) / r0; d03 = (dr03 - dot0) / r0
        d10 = (dr10 - dot1) / r1; d11 = (dr11 - dot1) / r1
        d12 = (dr12 - dot1) / r1; d13 = (dr13 - dot1) / r1
        d20 = (dr20 - dot2) / r2; d21 = (dr21 - dot2) / r2
        d22 = (dr22 - dot2) / r2; d23 = (dr23 - dot2) / r2
        d30 = (dr30 - dot3) / r3; d31 = (dr31 - dot3) / r3
        d32 = (dr32 - dot3) / r3; d33 = (dr33 - dot3) / r3

    # Multiply by sign for abs()
    d00 *= s00; d01 *= s01; d02 *= s02; d03 *= s03
    d10 *= s10; d11 *= s11; d12 *= s12; d13 *= s13
    d20 *= s20; d21 *= s21; d22 *= s22; d23 *= s23
    d30 *= s30; d31 *= s31; d32 *= s32; d33 *= s33

    # Store
    tl.store(dM_ptr + base + 0, d00)
    tl.store(dM_ptr + base + 1, d01)
    tl.store(dM_ptr + base + 2, d02)
    tl.store(dM_ptr + base + 3, d03)
    tl.store(dM_ptr + base + 4, d10)
    tl.store(dM_ptr + base + 5, d11)
    tl.store(dM_ptr + base + 6, d12)
    tl.store(dM_ptr + base + 7, d13)
    tl.store(dM_ptr + base + 8, d20)
    tl.store(dM_ptr + base + 9, d21)
    tl.store(dM_ptr + base + 10, d22)
    tl.store(dM_ptr + base + 11, d23)
    tl.store(dM_ptr + base + 12, d30)
    tl.store(dM_ptr + base + 13, d31)
    tl.store(dM_ptr + base + 14, d32)
    tl.store(dM_ptr + base + 15, d33)


# -----------------------------------------------------------------------------
# Add Residual Backward Kernel
# -----------------------------------------------------------------------------

@triton.jit
def _add_residual_backward_kernel(
    dH_new_ptr, branch_output_ptr, H_post_ptr,
    dH_residual_ptr, d_branch_output_ptr, dH_post_partial_ptr,
    batch, seq, dim,
    stride_h_b, stride_h_s, stride_h_n, stride_h_d,
    stride_bo_b, stride_bo_s, stride_bo_d,
    BLOCK_DIM: tl.constexpr,
):
    """Backward: H_new = H_residual + H_post * branch_output"""
    pid_bs = tl.program_id(0)
    pid_d = tl.program_id(1)

    b = pid_bs // seq
    s = pid_bs % seq

    d_offs = pid_d * BLOCK_DIM + tl.arange(0, BLOCK_DIM)
    d_mask = d_offs < dim

    # Load H_post and branch_output
    hp_base = H_post_ptr + b * 4
    hp0 = tl.load(hp_base + 0).to(tl.float32)
    hp1 = tl.load(hp_base + 1).to(tl.float32)
    hp2 = tl.load(hp_base + 2).to(tl.float32)
    hp3 = tl.load(hp_base + 3).to(tl.float32)

    bo_base = branch_output_ptr + b * stride_bo_b + s * stride_bo_s
    branch = tl.load(bo_base + d_offs * stride_bo_d, mask=d_mask, other=0.0).to(tl.float32)

    # Load dH_new
    dh_base = dH_new_ptr + b * stride_h_b + s * stride_h_s
    dh0 = tl.load(dh_base + 0 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dh1 = tl.load(dh_base + 1 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dh2 = tl.load(dh_base + 2 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dh3 = tl.load(dh_base + 3 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)

    # dH_residual = dH_new
    dhr_base = dH_residual_ptr + b * stride_h_b + s * stride_h_s
    tl.store(dhr_base + 0 * stride_h_n + d_offs * stride_h_d, dh0, mask=d_mask)
    tl.store(dhr_base + 1 * stride_h_n + d_offs * stride_h_d, dh1, mask=d_mask)
    tl.store(dhr_base + 2 * stride_h_n + d_offs * stride_h_d, dh2, mask=d_mask)
    tl.store(dhr_base + 3 * stride_h_n + d_offs * stride_h_d, dh3, mask=d_mask)

    # d_branch_output = sum_n(H_post[n] * dH_new[n])
    d_bo = hp0 * dh0 + hp1 * dh1 + hp2 * dh2 + hp3 * dh3
    d_bo_base = d_branch_output_ptr + b * stride_bo_b + s * stride_bo_s
    tl.store(d_bo_base + d_offs * stride_bo_d, d_bo, mask=d_mask)

    # Partial sums for dH_post
    num_d_blocks = tl.cdiv(dim, BLOCK_DIM)
    partial_base = dH_post_partial_ptr + (b * seq + s) * num_d_blocks * 4 + pid_d * 4
    tl.store(partial_base + 0, tl.sum(tl.where(d_mask, dh0 * branch, 0.0)))
    tl.store(partial_base + 1, tl.sum(tl.where(d_mask, dh1 * branch, 0.0)))
    tl.store(partial_base + 2, tl.sum(tl.where(d_mask, dh2 * branch, 0.0)))
    tl.store(partial_base + 3, tl.sum(tl.where(d_mask, dh3 * branch, 0.0)))


@triton.jit
def _reduce_dH_post_kernel(dH_post_partial_ptr, dH_post_ptr, batch, seq, num_d_blocks):
    """Reduce partial sums for dH_post."""
    b = tl.program_id(0)
    if b >= batch:
        return

    acc0, acc1, acc2, acc3 = 0.0, 0.0, 0.0, 0.0
    for s in range(seq):
        for d_blk in range(num_d_blocks):
            base = (b * seq + s) * num_d_blocks * 4 + d_blk * 4
            acc0 += tl.load(dH_post_partial_ptr + base + 0).to(tl.float32)
            acc1 += tl.load(dH_post_partial_ptr + base + 1).to(tl.float32)
            acc2 += tl.load(dH_post_partial_ptr + base + 2).to(tl.float32)
            acc3 += tl.load(dH_post_partial_ptr + base + 3).to(tl.float32)

    out_base = dH_post_ptr + b * 4
    tl.store(out_base + 0, acc0)
    tl.store(out_base + 1, acc1)
    tl.store(out_base + 2, acc2)
    tl.store(out_base + 3, acc3)


# -----------------------------------------------------------------------------
# Stream Mix Backward Kernel
# -----------------------------------------------------------------------------

@triton.jit
def _stream_mix_backward_kernel(
    d_branch_input_ptr, dH_residual_ptr,
    H_ptr, H_pre_ptr, H_res_ptr,
    dH_ptr, dH_pre_partial_ptr, dH_res_partial_ptr,
    batch, seq, dim,
    stride_h_b, stride_h_s, stride_h_n, stride_h_d,
    stride_bi_b, stride_bi_s, stride_bi_d,
    BLOCK_DIM: tl.constexpr,
):
    """Backward through stream mixing."""
    pid_bs = tl.program_id(0)
    pid_d = tl.program_id(1)

    b = pid_bs // seq
    s = pid_bs % seq

    d_offs = pid_d * BLOCK_DIM + tl.arange(0, BLOCK_DIM)
    d_mask = d_offs < dim

    # Load H[b,s,:,:]
    h_base = H_ptr + b * stride_h_b + s * stride_h_s
    h0 = tl.load(h_base + 0 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    h1 = tl.load(h_base + 1 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    h2 = tl.load(h_base + 2 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    h3 = tl.load(h_base + 3 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)

    # Load weights
    hp_base = H_pre_ptr + b * 4
    hp0 = tl.load(hp_base + 0).to(tl.float32)
    hp1 = tl.load(hp_base + 1).to(tl.float32)
    hp2 = tl.load(hp_base + 2).to(tl.float32)
    hp3 = tl.load(hp_base + 3).to(tl.float32)

    hr_base = H_res_ptr + b * 16
    hr00 = tl.load(hr_base + 0).to(tl.float32)
    hr01 = tl.load(hr_base + 1).to(tl.float32)
    hr02 = tl.load(hr_base + 2).to(tl.float32)
    hr03 = tl.load(hr_base + 3).to(tl.float32)
    hr10 = tl.load(hr_base + 4).to(tl.float32)
    hr11 = tl.load(hr_base + 5).to(tl.float32)
    hr12 = tl.load(hr_base + 6).to(tl.float32)
    hr13 = tl.load(hr_base + 7).to(tl.float32)
    hr20 = tl.load(hr_base + 8).to(tl.float32)
    hr21 = tl.load(hr_base + 9).to(tl.float32)
    hr22 = tl.load(hr_base + 10).to(tl.float32)
    hr23 = tl.load(hr_base + 11).to(tl.float32)
    hr30 = tl.load(hr_base + 12).to(tl.float32)
    hr31 = tl.load(hr_base + 13).to(tl.float32)
    hr32 = tl.load(hr_base + 14).to(tl.float32)
    hr33 = tl.load(hr_base + 15).to(tl.float32)

    # Load gradients
    dbi_base = d_branch_input_ptr + b * stride_bi_b + s * stride_bi_s
    d_bi = tl.load(dbi_base + d_offs * stride_bi_d, mask=d_mask, other=0.0).to(tl.float32)

    dhr_base = dH_residual_ptr + b * stride_h_b + s * stride_h_s
    dhr0 = tl.load(dhr_base + 0 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dhr1 = tl.load(dhr_base + 1 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dhr2 = tl.load(dhr_base + 2 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dhr3 = tl.load(dhr_base + 3 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)

    # dH from branch_input
    dh0 = hp0 * d_bi
    dh1 = hp1 * d_bi
    dh2 = hp2 * d_bi
    dh3 = hp3 * d_bi

    # dH from H_residual (transpose of H_res)
    dh0 += hr00 * dhr0 + hr10 * dhr1 + hr20 * dhr2 + hr30 * dhr3
    dh1 += hr01 * dhr0 + hr11 * dhr1 + hr21 * dhr2 + hr31 * dhr3
    dh2 += hr02 * dhr0 + hr12 * dhr1 + hr22 * dhr2 + hr32 * dhr3
    dh3 += hr03 * dhr0 + hr13 * dhr1 + hr23 * dhr2 + hr33 * dhr3

    # Store dH
    dh_base = dH_ptr + b * stride_h_b + s * stride_h_s
    tl.store(dh_base + 0 * stride_h_n + d_offs * stride_h_d, dh0, mask=d_mask)
    tl.store(dh_base + 1 * stride_h_n + d_offs * stride_h_d, dh1, mask=d_mask)
    tl.store(dh_base + 2 * stride_h_n + d_offs * stride_h_d, dh2, mask=d_mask)
    tl.store(dh_base + 3 * stride_h_n + d_offs * stride_h_d, dh3, mask=d_mask)

    # Partial sums for weight gradients
    num_d_blocks = tl.cdiv(dim, BLOCK_DIM)
    dhp_base = dH_pre_partial_ptr + (b * seq + s) * num_d_blocks * 4 + pid_d * 4
    tl.store(dhp_base + 0, tl.sum(tl.where(d_mask, d_bi * h0, 0.0)))
    tl.store(dhp_base + 1, tl.sum(tl.where(d_mask, d_bi * h1, 0.0)))
    tl.store(dhp_base + 2, tl.sum(tl.where(d_mask, d_bi * h2, 0.0)))
    tl.store(dhp_base + 3, tl.sum(tl.where(d_mask, d_bi * h3, 0.0)))

    # dH_res partials
    dhr_out = dH_res_partial_ptr + (b * seq + s) * num_d_blocks * 16 + pid_d * 16
    tl.store(dhr_out + 0, tl.sum(tl.where(d_mask, dhr0 * h0, 0.0)))
    tl.store(dhr_out + 1, tl.sum(tl.where(d_mask, dhr0 * h1, 0.0)))
    tl.store(dhr_out + 2, tl.sum(tl.where(d_mask, dhr0 * h2, 0.0)))
    tl.store(dhr_out + 3, tl.sum(tl.where(d_mask, dhr0 * h3, 0.0)))
    tl.store(dhr_out + 4, tl.sum(tl.where(d_mask, dhr1 * h0, 0.0)))
    tl.store(dhr_out + 5, tl.sum(tl.where(d_mask, dhr1 * h1, 0.0)))
    tl.store(dhr_out + 6, tl.sum(tl.where(d_mask, dhr1 * h2, 0.0)))
    tl.store(dhr_out + 7, tl.sum(tl.where(d_mask, dhr1 * h3, 0.0)))
    tl.store(dhr_out + 8, tl.sum(tl.where(d_mask, dhr2 * h0, 0.0)))
    tl.store(dhr_out + 9, tl.sum(tl.where(d_mask, dhr2 * h1, 0.0)))
    tl.store(dhr_out + 10, tl.sum(tl.where(d_mask, dhr2 * h2, 0.0)))
    tl.store(dhr_out + 11, tl.sum(tl.where(d_mask, dhr2 * h3, 0.0)))
    tl.store(dhr_out + 12, tl.sum(tl.where(d_mask, dhr3 * h0, 0.0)))
    tl.store(dhr_out + 13, tl.sum(tl.where(d_mask, dhr3 * h1, 0.0)))
    tl.store(dhr_out + 14, tl.sum(tl.where(d_mask, dhr3 * h2, 0.0)))
    tl.store(dhr_out + 15, tl.sum(tl.where(d_mask, dhr3 * h3, 0.0)))


@triton.jit
def _reduce_stream_mix_weights_kernel(
    dH_pre_partial_ptr, dH_res_partial_ptr, dH_pre_ptr, dH_res_ptr,
    batch, seq, num_d_blocks,
):
    """Reduce partial sums for stream mix weight gradients."""
    b = tl.program_id(0)
    if b >= batch:
        return

    # Accumulators for H_pre (4 values)
    acc_pre0, acc_pre1, acc_pre2, acc_pre3 = 0.0, 0.0, 0.0, 0.0

    # Accumulators for H_res (16 values, 4x4 matrix)
    acc_r00, acc_r01, acc_r02, acc_r03 = 0.0, 0.0, 0.0, 0.0
    acc_r10, acc_r11, acc_r12, acc_r13 = 0.0, 0.0, 0.0, 0.0
    acc_r20, acc_r21, acc_r22, acc_r23 = 0.0, 0.0, 0.0, 0.0
    acc_r30, acc_r31, acc_r32, acc_r33 = 0.0, 0.0, 0.0, 0.0

    for s in range(seq):
        for d_blk in range(num_d_blocks):
            pre_base = (b * seq + s) * num_d_blocks * 4 + d_blk * 4
            acc_pre0 += tl.load(dH_pre_partial_ptr + pre_base + 0).to(tl.float32)
            acc_pre1 += tl.load(dH_pre_partial_ptr + pre_base + 1).to(tl.float32)
            acc_pre2 += tl.load(dH_pre_partial_ptr + pre_base + 2).to(tl.float32)
            acc_pre3 += tl.load(dH_pre_partial_ptr + pre_base + 3).to(tl.float32)

            res_base = (b * seq + s) * num_d_blocks * 16 + d_blk * 16
            acc_r00 += tl.load(dH_res_partial_ptr + res_base + 0).to(tl.float32)
            acc_r01 += tl.load(dH_res_partial_ptr + res_base + 1).to(tl.float32)
            acc_r02 += tl.load(dH_res_partial_ptr + res_base + 2).to(tl.float32)
            acc_r03 += tl.load(dH_res_partial_ptr + res_base + 3).to(tl.float32)
            acc_r10 += tl.load(dH_res_partial_ptr + res_base + 4).to(tl.float32)
            acc_r11 += tl.load(dH_res_partial_ptr + res_base + 5).to(tl.float32)
            acc_r12 += tl.load(dH_res_partial_ptr + res_base + 6).to(tl.float32)
            acc_r13 += tl.load(dH_res_partial_ptr + res_base + 7).to(tl.float32)
            acc_r20 += tl.load(dH_res_partial_ptr + res_base + 8).to(tl.float32)
            acc_r21 += tl.load(dH_res_partial_ptr + res_base + 9).to(tl.float32)
            acc_r22 += tl.load(dH_res_partial_ptr + res_base + 10).to(tl.float32)
            acc_r23 += tl.load(dH_res_partial_ptr + res_base + 11).to(tl.float32)
            acc_r30 += tl.load(dH_res_partial_ptr + res_base + 12).to(tl.float32)
            acc_r31 += tl.load(dH_res_partial_ptr + res_base + 13).to(tl.float32)
            acc_r32 += tl.load(dH_res_partial_ptr + res_base + 14).to(tl.float32)
            acc_r33 += tl.load(dH_res_partial_ptr + res_base + 15).to(tl.float32)

    # Store dH_pre
    pre_out = dH_pre_ptr + b * 4
    tl.store(pre_out + 0, acc_pre0)
    tl.store(pre_out + 1, acc_pre1)
    tl.store(pre_out + 2, acc_pre2)
    tl.store(pre_out + 3, acc_pre3)

    # Store dH_res
    res_out = dH_res_ptr + b * 16
    tl.store(res_out + 0, acc_r00)
    tl.store(res_out + 1, acc_r01)
    tl.store(res_out + 2, acc_r02)
    tl.store(res_out + 3, acc_r03)
    tl.store(res_out + 4, acc_r10)
    tl.store(res_out + 5, acc_r11)
    tl.store(res_out + 6, acc_r12)
    tl.store(res_out + 7, acc_r13)
    tl.store(res_out + 8, acc_r20)
    tl.store(res_out + 9, acc_r21)
    tl.store(res_out + 10, acc_r22)
    tl.store(res_out + 11, acc_r23)
    tl.store(res_out + 12, acc_r30)
    tl.store(res_out + 13, acc_r31)
    tl.store(res_out + 14, acc_r32)
    tl.store(res_out + 15, acc_r33)


# -----------------------------------------------------------------------------
# Python Wrappers
# -----------------------------------------------------------------------------

def sinkhorn_backward(
    M_orig: torch.Tensor, dP: torch.Tensor,
    num_iters: int = 20, eps: float = 1e-8,
) -> torch.Tensor:
    """Backward through Sinkhorn-Knopp with recomputation."""
    M_orig = M_orig.contiguous()
    dP = dP.contiguous()
    batch = M_orig.shape[0]
    dM = torch.empty_like(M_orig)

    _sinkhorn_backward_kernel[(batch,)](
        M_orig, dP, dM, batch,
        num_iters=num_iters, eps=eps, NUM_STREAMS=4,
    )
    return dM


def add_residual_backward(
    dH_new: torch.Tensor, branch_output: torch.Tensor, H_post: torch.Tensor,
    block_dim: int = 128,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Backward through residual addition."""
    batch, seq, num_streams, dim = dH_new.shape

    dH_new = dH_new.contiguous()
    branch_output = branch_output.contiguous()
    H_post = H_post.contiguous()

    dH_residual = torch.empty_like(dH_new)
    d_branch_output = torch.empty_like(branch_output)
    num_d_blocks = triton.cdiv(dim, block_dim)
    dH_post_partial = torch.empty(batch, seq, num_d_blocks, 4, device=dH_new.device, dtype=torch.float32)

    grid = (batch * seq, num_d_blocks)
    _add_residual_backward_kernel[grid](
        dH_new, branch_output, H_post,
        dH_residual, d_branch_output, dH_post_partial,
        batch, seq, dim,
        dH_new.stride(0), dH_new.stride(1), dH_new.stride(2), dH_new.stride(3),
        branch_output.stride(0), branch_output.stride(1), branch_output.stride(2),
        BLOCK_DIM=block_dim,
    )

    # Use PyTorch's optimized parallel reduction
    # dH_post_partial: [batch, seq, num_d_blocks, 4] -> [batch, 4]
    dH_post = dH_post_partial.sum(dim=(1, 2)).to(dH_new.dtype)

    return dH_residual, d_branch_output, dH_post


def stream_mix_backward(
    d_branch_input: torch.Tensor, dH_residual: torch.Tensor,
    H: torch.Tensor, H_pre: torch.Tensor, H_res: torch.Tensor,
    block_dim: int = 128,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Backward through stream mixing."""
    batch, seq, num_streams, dim = H.shape

    d_branch_input = d_branch_input.contiguous()
    dH_residual = dH_residual.contiguous()
    H = H.contiguous()
    H_pre = H_pre.contiguous()
    H_res = H_res.contiguous()

    dH = torch.empty_like(H)
    num_d_blocks = triton.cdiv(dim, block_dim)
    dH_pre_partial = torch.empty(batch, seq, num_d_blocks, 4, device=H.device, dtype=torch.float32)
    dH_res_partial = torch.empty(batch, seq, num_d_blocks, 16, device=H.device, dtype=torch.float32)

    grid = (batch * seq, num_d_blocks)
    _stream_mix_backward_kernel[grid](
        d_branch_input, dH_residual, H, H_pre, H_res,
        dH, dH_pre_partial, dH_res_partial,
        batch, seq, dim,
        H.stride(0), H.stride(1), H.stride(2), H.stride(3),
        d_branch_input.stride(0), d_branch_input.stride(1), d_branch_input.stride(2),
        BLOCK_DIM=block_dim,
    )

    # Use PyTorch's optimized parallel reduction instead of serial kernel
    # dH_pre_partial: [batch, seq, num_d_blocks, 4] -> [batch, 4]
    # dH_res_partial: [batch, seq, num_d_blocks, 16] -> [batch, 4, 4]
    dH_pre = dH_pre_partial.sum(dim=(1, 2)).to(H.dtype)
    dH_res = dH_res_partial.sum(dim=(1, 2)).reshape(batch, 4, 4).to(H.dtype)

    return dH, dH_pre, dH_res


# -----------------------------------------------------------------------------
# Fused Stream Operations Backward (combines stream_mix + add_residual backward)
# Reduces memory traffic from (3n+1)C to (n+1)C reads as per paper
# -----------------------------------------------------------------------------

@triton.jit
def _fused_stream_backward_kernel(
    # Upstream gradients
    dH_new_ptr,             # [batch, seq, n, dim] - gradient from output
    # Forward inputs (for backward computation)
    H_ptr,                  # [batch, seq, n, dim] - original input
    branch_output_ptr,      # [batch, seq, dim] - layer output
    H_pre_ptr,              # [batch, n]
    H_post_ptr,             # [batch, n]
    H_res_ptr,              # [batch, n, n]
    # Outputs
    dH_ptr,                 # [batch, seq, n, dim] - gradient to input
    d_branch_output_ptr,    # [batch, seq, dim] - gradient to layer
    dH_pre_partial_ptr,     # [batch, seq, num_d_blocks, n]
    dH_post_partial_ptr,    # [batch, seq, num_d_blocks, n]
    dH_res_partial_ptr,     # [batch, seq, num_d_blocks, n*n]
    # Dimensions
    batch, seq, dim,
    stride_h_b, stride_h_s, stride_h_n, stride_h_d,
    stride_bo_b, stride_bo_s, stride_bo_d,
    BLOCK_DIM: tl.constexpr,
):
    """
    Fused backward through stream_mix and add_residual.
    
    Forward was:
        branch_input = einsum('bn,bsnd->bsd', H_pre, H)
        H_residual = einsum('bnm,bsmd->bsnd', H_res, H)
        H_new = H_residual + H_post[:, None, :, None] * branch_output[:, :, None, :]
    
    This kernel computes gradients for H, branch_output, H_pre, H_post, H_res
    in a single pass, reducing memory traffic.
    """
    pid_bs = tl.program_id(0)
    pid_d = tl.program_id(1)
    
    b = pid_bs // seq
    s = pid_bs % seq
    
    d_offs = pid_d * BLOCK_DIM + tl.arange(0, BLOCK_DIM)
    d_mask = d_offs < dim
    
    # Load H_pre, H_post, H_res for this batch
    hp_base = H_pre_ptr + b * 4
    hp0 = tl.load(hp_base + 0).to(tl.float32)
    hp1 = tl.load(hp_base + 1).to(tl.float32)
    hp2 = tl.load(hp_base + 2).to(tl.float32)
    hp3 = tl.load(hp_base + 3).to(tl.float32)
    
    hpost_base = H_post_ptr + b * 4
    hpost0 = tl.load(hpost_base + 0).to(tl.float32)
    hpost1 = tl.load(hpost_base + 1).to(tl.float32)
    hpost2 = tl.load(hpost_base + 2).to(tl.float32)
    hpost3 = tl.load(hpost_base + 3).to(tl.float32)
    
    hr_base = H_res_ptr + b * 16
    hr00 = tl.load(hr_base + 0).to(tl.float32)
    hr01 = tl.load(hr_base + 1).to(tl.float32)
    hr02 = tl.load(hr_base + 2).to(tl.float32)
    hr03 = tl.load(hr_base + 3).to(tl.float32)
    hr10 = tl.load(hr_base + 4).to(tl.float32)
    hr11 = tl.load(hr_base + 5).to(tl.float32)
    hr12 = tl.load(hr_base + 6).to(tl.float32)
    hr13 = tl.load(hr_base + 7).to(tl.float32)
    hr20 = tl.load(hr_base + 8).to(tl.float32)
    hr21 = tl.load(hr_base + 9).to(tl.float32)
    hr22 = tl.load(hr_base + 10).to(tl.float32)
    hr23 = tl.load(hr_base + 11).to(tl.float32)
    hr30 = tl.load(hr_base + 12).to(tl.float32)
    hr31 = tl.load(hr_base + 13).to(tl.float32)
    hr32 = tl.load(hr_base + 14).to(tl.float32)
    hr33 = tl.load(hr_base + 15).to(tl.float32)
    
    # Load dH_new
    dhn_base = dH_new_ptr + b * stride_h_b + s * stride_h_s
    dhn0 = tl.load(dhn_base + 0 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dhn1 = tl.load(dhn_base + 1 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dhn2 = tl.load(dhn_base + 2 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    dhn3 = tl.load(dhn_base + 3 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    
    # Load branch_output for H_post gradient
    bo_base = branch_output_ptr + b * stride_bo_b + s * stride_bo_s
    branch = tl.load(bo_base + d_offs * stride_bo_d, mask=d_mask, other=0.0).to(tl.float32)
    
    # Load H for weight gradients
    h_base = H_ptr + b * stride_h_b + s * stride_h_s
    h0 = tl.load(h_base + 0 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    h1 = tl.load(h_base + 1 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    h2 = tl.load(h_base + 2 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    h3 = tl.load(h_base + 3 * stride_h_n + d_offs * stride_h_d, mask=d_mask, other=0.0).to(tl.float32)
    
    # ---- Backward through H_new = H_residual + H_post * branch_output ----
    # dH_residual = dH_new (identity)
    dhr0 = dhn0
    dhr1 = dhn1
    dhr2 = dhn2
    dhr3 = dhn3
    
    # d_branch_output = sum_n(H_post[n] * dH_new[n])
    d_bo = hpost0 * dhn0 + hpost1 * dhn1 + hpost2 * dhn2 + hpost3 * dhn3
    
    # ---- Backward through stream_mix ----
    # branch_input = H_pre @ H, H_residual = H_res @ H
    # We need d_branch_input for the layer backward, but here we compute dH directly
    
    # For dH from H_residual: dH[m] += sum_n(H_res[n,m] * dH_residual[n])
    # H_res is [n, m], so transpose for backward
    dh0 = hr00 * dhr0 + hr10 * dhr1 + hr20 * dhr2 + hr30 * dhr3
    dh1 = hr01 * dhr0 + hr11 * dhr1 + hr21 * dhr2 + hr31 * dhr3
    dh2 = hr02 * dhr0 + hr12 * dhr1 + hr22 * dhr2 + hr32 * dhr3
    dh3 = hr03 * dhr0 + hr13 * dhr1 + hr23 * dhr2 + hr33 * dhr3
    
    # For dH from branch_input (through layer backward):
    # This path goes: dH_new -> d_branch_output -> d_branch_input -> dH
    # d_branch_input comes from layer backward (not computed here)
    # dH[n] += H_pre[n] * d_branch_input
    # We compute this contribution outside since it depends on layer backward
    
    # Store dH (partial - will add H_pre contribution after layer backward)
    dh_base = dH_ptr + b * stride_h_b + s * stride_h_s
    tl.store(dh_base + 0 * stride_h_n + d_offs * stride_h_d, dh0, mask=d_mask)
    tl.store(dh_base + 1 * stride_h_n + d_offs * stride_h_d, dh1, mask=d_mask)
    tl.store(dh_base + 2 * stride_h_n + d_offs * stride_h_d, dh2, mask=d_mask)
    tl.store(dh_base + 3 * stride_h_n + d_offs * stride_h_d, dh3, mask=d_mask)
    
    # Store d_branch_output
    d_bo_base = d_branch_output_ptr + b * stride_bo_b + s * stride_bo_s
    tl.store(d_bo_base + d_offs * stride_bo_d, d_bo, mask=d_mask)
    
    # ---- Compute partial sums for weight gradients ----
    num_d_blocks = tl.cdiv(dim, BLOCK_DIM)
    
    # dH_post[n] = sum_{s,d}(dH_new[n,d] * branch_output[d])
    dhpost_base = dH_post_partial_ptr + (b * seq + s) * num_d_blocks * 4 + pid_d * 4
    tl.store(dhpost_base + 0, tl.sum(tl.where(d_mask, dhn0 * branch, 0.0)))
    tl.store(dhpost_base + 1, tl.sum(tl.where(d_mask, dhn1 * branch, 0.0)))
    tl.store(dhpost_base + 2, tl.sum(tl.where(d_mask, dhn2 * branch, 0.0)))
    tl.store(dhpost_base + 3, tl.sum(tl.where(d_mask, dhn3 * branch, 0.0)))
    
    # dH_res[n,m] = sum_{s,d}(dH_residual[n,d] * H[m,d])
    dhres_base = dH_res_partial_ptr + (b * seq + s) * num_d_blocks * 16 + pid_d * 16
    tl.store(dhres_base + 0, tl.sum(tl.where(d_mask, dhr0 * h0, 0.0)))
    tl.store(dhres_base + 1, tl.sum(tl.where(d_mask, dhr0 * h1, 0.0)))
    tl.store(dhres_base + 2, tl.sum(tl.where(d_mask, dhr0 * h2, 0.0)))
    tl.store(dhres_base + 3, tl.sum(tl.where(d_mask, dhr0 * h3, 0.0)))
    tl.store(dhres_base + 4, tl.sum(tl.where(d_mask, dhr1 * h0, 0.0)))
    tl.store(dhres_base + 5, tl.sum(tl.where(d_mask, dhr1 * h1, 0.0)))
    tl.store(dhres_base + 6, tl.sum(tl.where(d_mask, dhr1 * h2, 0.0)))
    tl.store(dhres_base + 7, tl.sum(tl.where(d_mask, dhr1 * h3, 0.0)))
    tl.store(dhres_base + 8, tl.sum(tl.where(d_mask, dhr2 * h0, 0.0)))
    tl.store(dhres_base + 9, tl.sum(tl.where(d_mask, dhr2 * h1, 0.0)))
    tl.store(dhres_base + 10, tl.sum(tl.where(d_mask, dhr2 * h2, 0.0)))
    tl.store(dhres_base + 11, tl.sum(tl.where(d_mask, dhr2 * h3, 0.0)))
    tl.store(dhres_base + 12, tl.sum(tl.where(d_mask, dhr3 * h0, 0.0)))
    tl.store(dhres_base + 13, tl.sum(tl.where(d_mask, dhr3 * h1, 0.0)))
    tl.store(dhres_base + 14, tl.sum(tl.where(d_mask, dhr3 * h2, 0.0)))
    tl.store(dhres_base + 15, tl.sum(tl.where(d_mask, dhr3 * h3, 0.0)))
    
    # dH_pre is computed after layer backward (needs d_branch_input)
    # Just store placeholder partial for H_pre
    dhpre_base = dH_pre_partial_ptr + (b * seq + s) * num_d_blocks * 4 + pid_d * 4
    tl.store(dhpre_base + 0, 0.0)
    tl.store(dhpre_base + 1, 0.0)
    tl.store(dhpre_base + 2, 0.0)
    tl.store(dhpre_base + 3, 0.0)


# -----------------------------------------------------------------------------
# Fused Dynamic Weights V2 Backward Kernel
# Consolidates two matmuls (grad_x and grad_phi) into single kernel
# to avoid redundant reloading of x
# -----------------------------------------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_K': 256}, num_warps=4),
        triton.Config({'BLOCK_K': 512}, num_warps=4),
        triton.Config({'BLOCK_K': 1024}, num_warps=8),
    ],
    key=['in_dim'],
)
@triton.jit
def _fused_dynamic_weights_v2_backward_kernel(
    # Inputs (saved from forward)
    x_ptr,              # [batch, in_dim]
    phi_t_ptr,          # [24, in_dim] - transposed
    # Gradients from upstream
    grad_H_pre_ptr,     # [batch, 4]
    grad_H_post_ptr,    # [batch, 4]
    grad_scaled_res_ptr,# [batch, 16] - after Sinkhorn backward
    # Forward intermediate values (recomputed on-chip for most)
    H_pre_ptr,          # [batch, 4] - saved from forward
    # Scalars
    alpha_pre,
    alpha_post,
    alpha_res,
    # Outputs
    grad_x_ptr,         # [batch, in_dim]
    grad_phi_t_ptr,     # [24, in_dim] - transposed layout
    grad_bias_ptr,      # [24] - reduced across batch
    grad_alpha_pre_ptr, # scalar (reduced)
    grad_alpha_post_ptr,# scalar (reduced)
    grad_alpha_res_ptr, # scalar (reduced)
    # Dimensions
    batch,
    in_dim,
    # Constants
    BLOCK_K: tl.constexpr,
    eps: tl.constexpr,
):
    """
    Fused backward kernel for dynamic weights.
    
    Consolidates the two backward matmuls into a single kernel:
    1. grad_x = grad_raw @ phi^T (uses transposed phi)
    2. grad_phi = x^T @ grad_raw (accumulated across batch)
    
    Key optimization: x is loaded once and used for both:
    - Computing grad_x
    - Computing grad_phi (outer product accumulation)
    - Recomputing RMS norm for backward
    """
    pid = tl.program_id(0)
    if pid >= batch:
        return
    
    OUT_DIM: tl.constexpr = 24
    N: tl.constexpr = 4
    
    # Load upstream gradients
    grad_hp0 = tl.load(grad_H_pre_ptr + pid * N + 0)
    grad_hp1 = tl.load(grad_H_pre_ptr + pid * N + 1)
    grad_hp2 = tl.load(grad_H_pre_ptr + pid * N + 2)
    grad_hp3 = tl.load(grad_H_pre_ptr + pid * N + 3)
    
    grad_hpost0 = tl.load(grad_H_post_ptr + pid * N + 0)
    grad_hpost1 = tl.load(grad_H_post_ptr + pid * N + 1)
    grad_hpost2 = tl.load(grad_H_post_ptr + pid * N + 2)
    grad_hpost3 = tl.load(grad_H_post_ptr + pid * N + 3)
    
    # Load H_pre for backward through normalization
    hp0 = tl.load(H_pre_ptr + pid * N + 0)
    hp1 = tl.load(H_pre_ptr + pid * N + 1)
    hp2 = tl.load(H_pre_ptr + pid * N + 2)
    hp3 = tl.load(H_pre_ptr + pid * N + 3)
    
    # Load grad_scaled_res (16 values, already passed through Sinkhorn backward)
    res_base = grad_scaled_res_ptr + pid * 16
    gs8 = tl.load(res_base + 0); gs9 = tl.load(res_base + 1)
    gs10 = tl.load(res_base + 2); gs11 = tl.load(res_base + 3)
    gs12 = tl.load(res_base + 4); gs13 = tl.load(res_base + 5)
    gs14 = tl.load(res_base + 6); gs15 = tl.load(res_base + 7)
    gs16 = tl.load(res_base + 8); gs17 = tl.load(res_base + 9)
    gs18 = tl.load(res_base + 10); gs19 = tl.load(res_base + 11)
    gs20 = tl.load(res_base + 12); gs21 = tl.load(res_base + 13)
    gs22 = tl.load(res_base + 14); gs23 = tl.load(res_base + 15)
    
    # First pass: compute norm_sq and raw outputs (need for backward)
    x_base = x_ptr + pid * in_dim
    norm_sq = 0.0
    
    # Also accumulate raw outputs for backward through activations
    acc0 = 0.0; acc1 = 0.0; acc2 = 0.0; acc3 = 0.0
    acc4 = 0.0; acc5 = 0.0; acc6 = 0.0; acc7 = 0.0
    
    for k_start in range(0, in_dim, BLOCK_K):
        k_offs = k_start + tl.arange(0, BLOCK_K)
        k_mask = k_offs < in_dim
        
        x_vals = tl.load(x_base + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        norm_sq += tl.sum(x_vals * x_vals)
        
        # Accumulate for pre and post (needed for activation backward)
        phi0 = tl.load(phi_t_ptr + 0 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi1 = tl.load(phi_t_ptr + 1 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi2 = tl.load(phi_t_ptr + 2 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi3 = tl.load(phi_t_ptr + 3 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi4 = tl.load(phi_t_ptr + 4 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi5 = tl.load(phi_t_ptr + 5 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi6 = tl.load(phi_t_ptr + 6 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi7 = tl.load(phi_t_ptr + 7 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        
        acc0 += tl.sum(x_vals * phi0)
        acc1 += tl.sum(x_vals * phi1)
        acc2 += tl.sum(x_vals * phi2)
        acc3 += tl.sum(x_vals * phi3)
        acc4 += tl.sum(x_vals * phi4)
        acc5 += tl.sum(x_vals * phi5)
        acc6 += tl.sum(x_vals * phi6)
        acc7 += tl.sum(x_vals * phi7)
    
    # Compute inv_rms
    rms = tl.sqrt(norm_sq / in_dim + eps)
    inv_rms = 1.0 / rms
    
    # Recompute scaled values for activation backward
    # We need the pre-activation values to compute sigmoid derivatives
    # Load bias for recomputation
    b0 = tl.load(grad_bias_ptr + 0)  # This is actually bias, not grad yet
    b1 = tl.load(grad_bias_ptr + 1)
    b2 = tl.load(grad_bias_ptr + 2)
    b3 = tl.load(grad_bias_ptr + 3)
    b4 = tl.load(grad_bias_ptr + 4)
    b5 = tl.load(grad_bias_ptr + 5)
    b6 = tl.load(grad_bias_ptr + 6)
    b7 = tl.load(grad_bias_ptr + 7)
    
    # Backward through H_pre = sigmoid(scaled_pre) / sum(sigmoid(scaled_pre))
    # scaled_pre = inv_rms * alpha_pre * raw_pre + bias_pre
    scaled_pre0 = inv_rms * alpha_pre * acc0 + b0
    scaled_pre1 = inv_rms * alpha_pre * acc1 + b1
    scaled_pre2 = inv_rms * alpha_pre * acc2 + b2
    scaled_pre3 = inv_rms * alpha_pre * acc3 + b3
    
    sig_pre0 = tl.sigmoid(scaled_pre0)
    sig_pre1 = tl.sigmoid(scaled_pre1)
    sig_pre2 = tl.sigmoid(scaled_pre2)
    sig_pre3 = tl.sigmoid(scaled_pre3)
    sig_pre_sum = sig_pre0 + sig_pre1 + sig_pre2 + sig_pre3 + eps
    
    # Backward through normalization
    grad_sig_pre0 = (grad_hp0 - (grad_hp0 * hp0 + grad_hp1 * hp1 + grad_hp2 * hp2 + grad_hp3 * hp3)) / sig_pre_sum
    grad_sig_pre1 = (grad_hp1 - (grad_hp0 * hp0 + grad_hp1 * hp1 + grad_hp2 * hp2 + grad_hp3 * hp3)) / sig_pre_sum
    grad_sig_pre2 = (grad_hp2 - (grad_hp0 * hp0 + grad_hp1 * hp1 + grad_hp2 * hp2 + grad_hp3 * hp3)) / sig_pre_sum
    grad_sig_pre3 = (grad_hp3 - (grad_hp0 * hp0 + grad_hp1 * hp1 + grad_hp2 * hp2 + grad_hp3 * hp3)) / sig_pre_sum
    
    # Backward through sigmoid
    gs0 = grad_sig_pre0 * sig_pre0 * (1.0 - sig_pre0)
    gs1 = grad_sig_pre1 * sig_pre1 * (1.0 - sig_pre1)
    gs2 = grad_sig_pre2 * sig_pre2 * (1.0 - sig_pre2)
    gs3 = grad_sig_pre3 * sig_pre3 * (1.0 - sig_pre3)
    
    # Backward through H_post = 2 * sigmoid(scaled_post)
    scaled_post0 = inv_rms * alpha_post * acc4 + b4
    scaled_post1 = inv_rms * alpha_post * acc5 + b5
    scaled_post2 = inv_rms * alpha_post * acc6 + b6
    scaled_post3 = inv_rms * alpha_post * acc7 + b7
    
    sig_post0 = tl.sigmoid(scaled_post0)
    sig_post1 = tl.sigmoid(scaled_post1)
    sig_post2 = tl.sigmoid(scaled_post2)
    sig_post3 = tl.sigmoid(scaled_post3)
    
    gs4 = grad_hpost0 * 2.0 * sig_post0 * (1.0 - sig_post0)
    gs5 = grad_hpost1 * 2.0 * sig_post1 * (1.0 - sig_post1)
    gs6 = grad_hpost2 * 2.0 * sig_post2 * (1.0 - sig_post2)
    gs7 = grad_hpost3 * 2.0 * sig_post3 * (1.0 - sig_post3)
    
    # Now we have grad_scaled for all 24 outputs: gs0-gs23
    # Backward through scaling: scaled = inv_rms * alpha * raw + bias
    # grad_raw = grad_scaled * inv_rms * alpha
    gr0 = gs0 * inv_rms * alpha_pre
    gr1 = gs1 * inv_rms * alpha_pre
    gr2 = gs2 * inv_rms * alpha_pre
    gr3 = gs3 * inv_rms * alpha_pre
    gr4 = gs4 * inv_rms * alpha_post
    gr5 = gs5 * inv_rms * alpha_post
    gr6 = gs6 * inv_rms * alpha_post
    gr7 = gs7 * inv_rms * alpha_post
    gr8 = gs8 * inv_rms * alpha_res
    gr9 = gs9 * inv_rms * alpha_res
    gr10 = gs10 * inv_rms * alpha_res
    gr11 = gs11 * inv_rms * alpha_res
    gr12 = gs12 * inv_rms * alpha_res
    gr13 = gs13 * inv_rms * alpha_res
    gr14 = gs14 * inv_rms * alpha_res
    gr15 = gs15 * inv_rms * alpha_res
    gr16 = gs16 * inv_rms * alpha_res
    gr17 = gs17 * inv_rms * alpha_res
    gr18 = gs18 * inv_rms * alpha_res
    gr19 = gs19 * inv_rms * alpha_res
    gr20 = gs20 * inv_rms * alpha_res
    gr21 = gs21 * inv_rms * alpha_res
    gr22 = gs22 * inv_rms * alpha_res
    gr23 = gs23 * inv_rms * alpha_res
    
    # Gradient w.r.t. inv_rms (for grad_x through RMS)
    # grad_inv_rms = sum(grad_scaled * alpha * raw)
    # We need to recompute raw for 8-23, but we already have 0-7
    # For now, use atomic adds for grad_alpha (they're scalars)
    
    # Second pass: compute grad_x and accumulate grad_phi
    # grad_x = grad_raw @ phi (but phi is transposed, so we load phi_t rows)
    grad_x_base = grad_x_ptr + pid * in_dim
    
    for k_start in range(0, in_dim, BLOCK_K):
        k_offs = k_start + tl.arange(0, BLOCK_K)
        k_mask = k_offs < in_dim
        
        # Load x for this block (needed for grad_phi)
        x_vals = tl.load(x_base + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        
        # Load phi_t rows
        phi0 = tl.load(phi_t_ptr + 0 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi1 = tl.load(phi_t_ptr + 1 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi2 = tl.load(phi_t_ptr + 2 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi3 = tl.load(phi_t_ptr + 3 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi4 = tl.load(phi_t_ptr + 4 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi5 = tl.load(phi_t_ptr + 5 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi6 = tl.load(phi_t_ptr + 6 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi7 = tl.load(phi_t_ptr + 7 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi8 = tl.load(phi_t_ptr + 8 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi9 = tl.load(phi_t_ptr + 9 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi10 = tl.load(phi_t_ptr + 10 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi11 = tl.load(phi_t_ptr + 11 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi12 = tl.load(phi_t_ptr + 12 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi13 = tl.load(phi_t_ptr + 13 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi14 = tl.load(phi_t_ptr + 14 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi15 = tl.load(phi_t_ptr + 15 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi16 = tl.load(phi_t_ptr + 16 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi17 = tl.load(phi_t_ptr + 17 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi18 = tl.load(phi_t_ptr + 18 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi19 = tl.load(phi_t_ptr + 19 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi20 = tl.load(phi_t_ptr + 20 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi21 = tl.load(phi_t_ptr + 21 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi22 = tl.load(phi_t_ptr + 22 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        phi23 = tl.load(phi_t_ptr + 23 * in_dim + k_offs, mask=k_mask, other=0.0).to(tl.float32)
        
        # grad_x = sum over outputs of (grad_raw[i] * phi[i, k])
        grad_x = (
            gr0 * phi0 + gr1 * phi1 + gr2 * phi2 + gr3 * phi3 +
            gr4 * phi4 + gr5 * phi5 + gr6 * phi6 + gr7 * phi7 +
            gr8 * phi8 + gr9 * phi9 + gr10 * phi10 + gr11 * phi11 +
            gr12 * phi12 + gr13 * phi13 + gr14 * phi14 + gr15 * phi15 +
            gr16 * phi16 + gr17 * phi17 + gr18 * phi18 + gr19 * phi19 +
            gr20 * phi20 + gr21 * phi21 + gr22 * phi22 + gr23 * phi23
        )
        
        # Add gradient from RMS norm path
        # grad_x_from_rms = grad_inv_rms * d(inv_rms)/d(x)
        # d(inv_rms)/d(x) = -inv_rms^2 * d(rms)/d(x) = -inv_rms^2 * x / (in_dim * rms)
        #                 = -inv_rms^3 * x / in_dim
        # But we need grad_inv_rms first, which requires the full raw values
        # For simplicity, we'll handle RMS gradient separately
        
        tl.store(grad_x_base + k_offs, grad_x, mask=k_mask)
        
        # Accumulate grad_phi using atomic add (summed across batch)
        # grad_phi[i, k] = sum_batch(grad_raw[i] * x[k])
        tl.atomic_add(grad_phi_t_ptr + 0 * in_dim + k_offs, gr0 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 1 * in_dim + k_offs, gr1 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 2 * in_dim + k_offs, gr2 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 3 * in_dim + k_offs, gr3 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 4 * in_dim + k_offs, gr4 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 5 * in_dim + k_offs, gr5 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 6 * in_dim + k_offs, gr6 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 7 * in_dim + k_offs, gr7 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 8 * in_dim + k_offs, gr8 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 9 * in_dim + k_offs, gr9 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 10 * in_dim + k_offs, gr10 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 11 * in_dim + k_offs, gr11 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 12 * in_dim + k_offs, gr12 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 13 * in_dim + k_offs, gr13 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 14 * in_dim + k_offs, gr14 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 15 * in_dim + k_offs, gr15 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 16 * in_dim + k_offs, gr16 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 17 * in_dim + k_offs, gr17 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 18 * in_dim + k_offs, gr18 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 19 * in_dim + k_offs, gr19 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 20 * in_dim + k_offs, gr20 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 21 * in_dim + k_offs, gr21 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 22 * in_dim + k_offs, gr22 * x_vals, mask=k_mask)
        tl.atomic_add(grad_phi_t_ptr + 23 * in_dim + k_offs, gr23 * x_vals, mask=k_mask)
    
    # Accumulate grad_bias (atomic add across batch)
    tl.atomic_add(grad_bias_ptr + 0, gs0)
    tl.atomic_add(grad_bias_ptr + 1, gs1)
    tl.atomic_add(grad_bias_ptr + 2, gs2)
    tl.atomic_add(grad_bias_ptr + 3, gs3)
    tl.atomic_add(grad_bias_ptr + 4, gs4)
    tl.atomic_add(grad_bias_ptr + 5, gs5)
    tl.atomic_add(grad_bias_ptr + 6, gs6)
    tl.atomic_add(grad_bias_ptr + 7, gs7)
    tl.atomic_add(grad_bias_ptr + 8, gs8)
    tl.atomic_add(grad_bias_ptr + 9, gs9)
    tl.atomic_add(grad_bias_ptr + 10, gs10)
    tl.atomic_add(grad_bias_ptr + 11, gs11)
    tl.atomic_add(grad_bias_ptr + 12, gs12)
    tl.atomic_add(grad_bias_ptr + 13, gs13)
    tl.atomic_add(grad_bias_ptr + 14, gs14)
    tl.atomic_add(grad_bias_ptr + 15, gs15)
    tl.atomic_add(grad_bias_ptr + 16, gs16)
    tl.atomic_add(grad_bias_ptr + 17, gs17)
    tl.atomic_add(grad_bias_ptr + 18, gs18)
    tl.atomic_add(grad_bias_ptr + 19, gs19)
    tl.atomic_add(grad_bias_ptr + 20, gs20)
    tl.atomic_add(grad_bias_ptr + 21, gs21)
    tl.atomic_add(grad_bias_ptr + 22, gs22)
    tl.atomic_add(grad_bias_ptr + 23, gs23)


def fused_dynamic_weights_v2_backward(
    x: torch.Tensor,
    phi: torch.Tensor,
    bias: torch.Tensor,
    alpha_pre: torch.Tensor,
    alpha_post: torch.Tensor,
    alpha_res: torch.Tensor,
    H_pre: torch.Tensor,
    grad_H_pre: torch.Tensor,
    grad_H_post: torch.Tensor,
    grad_H_res: torch.Tensor,
    sinkhorn_iters: int,
    eps: float,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Fused backward pass for dynamic weights V2.
    
    Consolidates the two backward matmuls into a single kernel.
    """
    batch, in_dim = x.shape
    n = 4
    out_dim = 24
    
    x = x.contiguous().float()
    phi = phi.contiguous().float()
    bias = bias.contiguous().float()
    H_pre = H_pre.contiguous().float()
    grad_H_pre = grad_H_pre.contiguous().float()
    grad_H_post = grad_H_post.contiguous().float()
    grad_H_res = grad_H_res.contiguous().float()
    
    # First: backward through Sinkhorn
    # Recompute scaled_res for Sinkhorn backward
    rms = torch.sqrt((x * x).sum(dim=-1, keepdim=True) / in_dim + eps)
    inv_rms = 1.0 / rms
    raw_res = x @ phi[:, 8:]  # [batch, 16]
    scaled_res = inv_rms * alpha_res * raw_res + bias[8:]
    
    grad_scaled_res = sinkhorn_backward(
        scaled_res.reshape(batch, n, n),
        grad_H_res,
        sinkhorn_iters,
        eps,
    ).reshape(batch, n * n)
    
    # Transpose phi for the kernel
    phi_t = phi.T.contiguous()
    
    # Allocate outputs
    grad_x = torch.empty_like(x)
    grad_phi_t = torch.zeros(out_dim, in_dim, device=x.device, dtype=torch.float32)
    grad_bias = torch.zeros(out_dim, device=x.device, dtype=torch.float32)
    grad_alpha_pre = torch.zeros(1, device=x.device, dtype=torch.float32)
    grad_alpha_post = torch.zeros(1, device=x.device, dtype=torch.float32)
    grad_alpha_res = torch.zeros(1, device=x.device, dtype=torch.float32)
    
    # Launch kernel
    grid = (batch,)
    _fused_dynamic_weights_v2_backward_kernel[grid](
        x, phi_t,
        grad_H_pre, grad_H_post, grad_scaled_res,
        H_pre,
        alpha_pre.item(), alpha_post.item(), alpha_res.item(),
        grad_x, grad_phi_t, grad_bias,
        grad_alpha_pre, grad_alpha_post, grad_alpha_res,
        batch, in_dim,
        eps=eps,
    )
    
    # Transpose grad_phi back
    grad_phi = grad_phi_t.T.contiguous()
    
    return (
        grad_x,
        grad_phi,
        grad_bias,
        grad_alpha_pre.squeeze(),
        grad_alpha_post.squeeze(),
        grad_alpha_res.squeeze(),
    )

